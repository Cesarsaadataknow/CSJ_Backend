
#==================================================================================

import uuid
from fastapi import APIRouter, UploadFile, File, Form
from pydantic import BaseModel
from datetime import datetime
from core.retrieval import retrieve_from_index
from core.ai_services import AIServices 
from helpers.document_loader import extract_text_from_file
from helpers.word_writer import generate_word, upload_to_onelake
from helpers.prompts import build_prompt
from app.config import settings



# -----------------------------------------------------------------------------
# region               INICIALIZACI√ìN Y CONFIGURACI√ìN
# -----------------------------------------------------------------------------
# Instancias de servicios
cosmos_db = AIServices.AzureCosmosDB()
chat_router = APIRouter(tags=["chat"])
# endregion


# =====================================================
# MODELO PARA SWAGGER / JSON
# =====================================================
class ChatJSONRequest(BaseModel):
    question: str
    session_id: str | None = None
    user_id: str | None = None   # opcional si lo tienes


# =====================================================
# ENDPOINT PRINCIPAL (multipart/form-data)
# =====================================================
@chat_router.post("/")
async def chat(
    question: str = Form(...),
    session_id: str | None = Form(default=None),   
    user_id: str | None = Form(default=None),    
    files: list[UploadFile] | None = File(default=None)
):
    return await _process_chat(question, files, session_id=session_id, user_id=user_id)


# =====================================================
# ENDPOINT PARA SWAGGER (application/json)
# =====================================================
@chat_router.post("/json")
async def chat_json(payload: ChatJSONRequest):
    return await _process_chat(payload.question, files=None, session_id=payload.session_id, user_id=payload.user_id)


# =====================================================
# ENDPOINT PARA SWAGGER (upload)
# =====================================================
@chat_router.post("/upload")
async def chat_upload(
    question: str = Form(...),
    session_id: str | None = Form(default=None),  
    user_id: str | None = Form(default=None),     
    files: list[UploadFile] = File(...)
):
    return await _process_chat(question, files, session_id=session_id, user_id=user_id)


# =====================================================
# L√ìGICA CENTRAL (REUTILIZABLE)
# =====================================================
async def _process_chat(
    question: str,
    files: list[UploadFile] | None,
    session_id: str | None,
    user_id: str | None
):
    # 0) Asegurar session_id
    if not session_id:
        session_id = str(uuid.uuid4())

    # -------------------------------------------------
    # 1Ô∏è‚É£ Texto de documentos cargados
    # -------------------------------------------------
    uploaded_text = ""
    uploaded_files = []

    if files:
        for file in files:
            extracted = extract_text_from_file(file)  # SIN await
            uploaded_text += f"\n\n[DOCUMENTO: {file.filename}]\n{extracted}"
            uploaded_files.append(file.filename)

    # -------------------------------------------------
    # 2Ô∏è‚É£ Recuperar desde √≠ndice
    # -------------------------------------------------
    retrieved_docs = retrieve_from_index(question)

    index_context = ""
    citations = []
    retrieved_ids = []

    for i, d in enumerate(retrieved_docs, 1):
        texto = d.get("texto", "").strip()
        if not texto:
            continue

        index_context += f"[√çNDICE {i}]\n{texto}\n\n"
        citations.append(f"[√çNDICE {i}] ID: {d.get('id')}")
        retrieved_ids.append(d.get("id"))

    # Caso sin contexto: tambi√©n lo guardamos en Cosmos
    if not index_context.strip() and not uploaded_text.strip():
        no_info_response = {
            "answer": "No se encontr√≥ informaci√≥n suficiente en el √≠ndice ni en los documentos cargados.",
            "citations": [],
            "session_id": session_id,
        }

        cosmos_db.save_answer_rag(
            session_id=session_id,
            user_id=user_id,
            user_question=question,
            ai_response=no_info_response["answer"],  # string
            citations=[],
            file_path=None,
            channel="web",
            extra={
                "uploaded_files": uploaded_files,
                "retrieved_ids": retrieved_ids,
                "status": "no_context"
            }
        )

        return no_info_response

    # -------------------------------------------------
    # 3Ô∏è‚É£ Contexto unificado (RAG)
    # -------------------------------------------------
    full_context = f"""
DOCUMENTOS DEL √çNDICE (JURISPRUDENCIA):
{index_context}

DOCUMENTOS CARGADOS POR EL USUARIO:
{uploaded_text}
"""

    # -------------------------------------------------
    # 4Ô∏è‚É£ Azure OpenAI
    # -------------------------------------------------
    system_prompt = (
        "Eres un asistente jur√≠dico experto en resoluci√≥n de conflictos de competencias. "
        "Responde exclusivamente con base en los documentos proporcionados. "
        "Utiliza lenguaje jur√≠dico formal y preciso."
    )

    client = AIServices.chat_client()

    # -------------------------------------------------
    # 5Ô∏è‚É£ Generar secciones jur√≠dicas
    # -------------------------------------------------
    sections = {}

    section_map = [
        ("I. ANTECEDENTES", "antecedentes"),
        ("II. CONSIDERACIONES", "consideraciones"),
        ("III. PROBLEMA JUR√çDICO", "problema"),
        ("IV. DECISI√ìN", "decision"),
    ]

    for title, key in section_map:
        completion = client.chat.completions.create(
            model=settings.AZURE_OPENAI_CHAT_DEPLOYMENT,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": build_prompt(title, full_context)},
            ],
            temperature=0,
        )

        sections[key] = completion.choices[0].message.content

    # -------------------------------------------------
    # 6Ô∏è‚É£ Generar Word
    # -------------------------------------------------
    docx_bytes = generate_word(
        template_path="templates/providencia.docx",
        content=sections
    )

    # destino en OneLake
    folder = f"documentos_generados/"
    filename = f"providencia_{session_id}.docx"

    WORKSPACE_NAME = "WS_Resolucion_Conflictos_Competencias_Administrativas"      # <-- pon el nombre real del workspace de Fabric
    LAKEHOUSE_NAME = "csj_documentos"    # <-- por tu screenshot

    onelake_path = upload_to_onelake(
        workspace_name=WORKSPACE_NAME,
        lakehouse_name=LAKEHOUSE_NAME,
        folder=folder,
        filename=filename,
        content_bytes=docx_bytes
    )

    # -------------------------------------------------
    # 7Ô∏è‚É£ Guardar en Cosmos (session + message)
    # -------------------------------------------------
    cosmos_db.save_answer_rag(
        session_id=session_id,
        user_id=user_id,
        user_question=question,
        ai_response=sections,         
        citations=citations,
        file_path=onelake_path,
        channel="web",
        extra={
            "uploaded_files": uploaded_files,
            "retrieved_ids": retrieved_ids,
            "status": "ok"
        }
    )

    # -------------------------------------------------
    # 8Ô∏è‚É£ Respuesta final
    # -------------------------------------------------
    return {
        "answer": sections,
        "citations": citations,
        "file": onelake_path,
        "session_id": session_id,  # üëà SUPER IMPORTANTE
    }


# #===========================================================================================
# from fastapi import APIRouter, UploadFile, File, Form
# from core.retrieval import retrieve_from_index
# from core.ai_services import AIServices
# from helpers.document_loader import extract_text_from_file
# from helpers.word_writer import generate_word
# from helpers.prompts import build_prompt
# from app.config import settings

# chat_router = APIRouter(tags=["chat"])


# @chat_router.post("/")
# async def chat(
#     question: str = Form(...),
#     files: list[UploadFile] | None = File(default=None)
# ):
#     # -------------------------------------------------
#     # 1Ô∏è‚É£ Texto de documentos cargados (PDF / Word)
#     # -------------------------------------------------
#     uploaded_text = ""

#     if files:
#         for file in files:
#             extracted = extract_text_from_file(file)  # SIN await
#             uploaded_text += f"\n\n[DOCUMENTO: {file.filename}]\n{extracted}"

#     # -------------------------------------------------
#     # 2Ô∏è‚É£ Recuperar desde √≠ndice (Fabric / AI Search)
#     # -------------------------------------------------
#     retrieved_docs = retrieve_from_index(question)

#     index_context = ""
#     citations = []

#     for i, d in enumerate(retrieved_docs, 1):
#         texto = d.get("texto", "").strip()
#         if not texto:
#             continue

#         index_context += f"[√çNDICE {i}]\n{texto}\n\n"
#         citations.append(f"[√çNDICE {i}] ID: {d.get('id')}")

#     if not index_context.strip() and not uploaded_text.strip():
#         return {
#             "answer": "No se encontr√≥ informaci√≥n suficiente en el √≠ndice ni en los documentos cargados.",
#             "citations": []
#         }

#     # -------------------------------------------------
#     # 3Ô∏è‚É£ Contexto unificado (RAG)
#     # -------------------------------------------------
#     full_context = f"""
# DOCUMENTOS DEL √çNDICE (JURISPRUDENCIA):
# {index_context}

# DOCUMENTOS CARGADOS POR EL USUARIO:
# {uploaded_text}
# """

#     # -------------------------------------------------
#     # 4Ô∏è‚É£ Azure OpenAI
#     # -------------------------------------------------
#     system_prompt = (
#         "Eres un asistente jur√≠dico experto en resoluci√≥n de conflictos de competencias. "
#         "Responde exclusivamente con base en los documentos proporcionados. "
#         "Utiliza lenguaje jur√≠dico formal y preciso."
#     )

#     client = AIServices.chat_client()

#     # -------------------------------------------------
#     # 5Ô∏è‚É£ Generar secciones jur√≠dicas
#     # -------------------------------------------------
#     sections = {}

#     section_map = [
#         ("I. ANTECEDENTES", "antecedentes"),
#         ("II. CONSIDERACIONES", "consideraciones"),
#         ("III. PROBLEMA JUR√çDICO", "problema"),
#         ("IV. DECISI√ìN", "decision"),
#     ]

#     for title, key in section_map:
#         completion = client.chat.completions.create(
#             model=settings.AZURE_OPENAI_CHAT_DEPLOYMENT,
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": build_prompt(title, full_context)},
#             ],
#             temperature=0,
#         )

#         sections[key] = completion.choices[0].message.content

#     # -------------------------------------------------
#     # 6Ô∏è‚É£ Generar Word desde plantilla
#     # -------------------------------------------------
#     output_path = "output/providencia_generada.docx"

#     generate_word(
#         template_path="templates/providencia.docx",
#         output_path=output_path,
#         content=sections
#     )

#     # -------------------------------------------------
#     # 7Ô∏è‚É£ Respuesta final
#     # -------------------------------------------------
#     return {
#         "answer": sections,
#         "citations": citations,
#         "file": output_path
#     }

#=======================================================================================================00

# from fastapi import APIRouter, UploadFile, File, Form
# from core.retrieval import retrieve_from_index
# from core.ai_services import AIServices
# from helpers.document_loader import extract_text_from_file
# from app.config import settings

# chat_router = APIRouter(tags=["chat"])

# @chat_router.post("/")
# async def chat(
#     question: str = Form(...),
#     files: list[UploadFile] | None = File(default=None)
# ):
#     # -------------------------------------------------
#     # 1Ô∏è‚É£ Texto de documentos cargados (PDF / Word)
#     # -------------------------------------------------
#     uploaded_text = ""

#     if files:
#         for file in files:
#             # extract_text_from_file ES SINCRONA ‚Üí NO await
#             extracted = extract_text_from_file(file)
#             uploaded_text += f"\n\n[DOCUMENTO: {file.filename}]\n{extracted}"

#     # -------------------------------------------------
#     # 2Ô∏è‚É£ Recuperar desde √≠ndice (Fabric / AI Search)
#     # -------------------------------------------------
#     retrieved_docs = retrieve_from_index(question)

#     index_context = ""
#     citations = []

#     for i, d in enumerate(retrieved_docs, 1):
#         texto = d.get("texto", "").strip()
#         if not texto:
#             continue

#         index_context += f"[√çNDICE {i}]\n{texto}\n\n"
#         citations.append(f"[√çNDICE {i}] ID: {d.get('id')}")

#     if not index_context.strip() and not uploaded_text.strip():
#         return {
#             "answer": "No se encontr√≥ informaci√≥n suficiente en el √≠ndice ni en los documentos cargados.",
#             "citations": []
#         }

#     # -------------------------------------------------
#     # 3Ô∏è‚É£ Prompt (RAG + Documentos)
#     # -------------------------------------------------
#     system_prompt = (
#         "Eres un asistente jur√≠dico experto en resoluci√≥n de conflictos de competencias. "
#         "Responde exclusivamente con base en los documentos proporcionados. "
#         "Utiliza lenguaje jur√≠dico formal y preciso."
#     )

#     user_prompt = f"""
# DOCUMENTOS DEL √çNDICE (JURISPRUDENCIA):
# {index_context}

# DOCUMENTOS CARGADOS POR EL USUARIO:
# {uploaded_text}

# PREGUNTA:
# {question}
# """

#     # -------------------------------------------------
#     # 4Ô∏è‚É£ Llamada a Azure OpenAI
#     # -------------------------------------------------
#     client = AIServices.chat_client()

#     completion = client.chat.completions.create(
#         model=settings.AZURE_OPENAI_CHAT_DEPLOYMENT,
#         messages=[
#             {"role": "system", "content": system_prompt},
#             {"role": "user", "content": user_prompt},
#         ],
#         temperature=0,
#     )

#     answer = completion.choices[0].message.content

#     return {
#         "answer": answer,
#         "citations": citations,
#     }

#-------------------------------------------------------------------------------------------------------------

# from fastapi import APIRouter, UploadFile, File, Form
# from core.retrieval import retrieve_from_index
# from core.ai_services import AIServices
# from helpers.document_loader import extract_text_from_file
# from app.config import settings


# chat_router = APIRouter(tags=["chat"])

# TEXTOPROVIDENCIA = """
# (TEXTO BASE DE PROVIDENCIA AQU√ç)
# """

# @chat_router.post("/")
# async def chat(
#     question: str = Form(...),
#     files: list[UploadFile] | None = File(default=None)
# ):
#     # 1Ô∏è‚É£ Texto de documentos cargados
#     uploaded_text = ""

#     if files:
#         for file in files:
#             extracted = await extract_text_from_file(file)
#             uploaded_text += f"\n\n[DOCUMENTO: {file.filename}]\n{extracted}"

#     # 2Ô∏è‚É£ Recuperar desde √≠ndice (Fabric / AI Search)
#     retrieved_docs = retrieve_from_index(question)

#     index_context = ""
#     citations = []

#     for i, d in enumerate(retrieved_docs, 1):
#         if d["texto"].strip():
#             index_context += f"[√çNDICE {i}]\n{d['texto']}\n\n"
#             citations.append(f"[√çNDICE {i}] ID: {d['id']}")

#     # 3Ô∏è‚É£ Prompt final (RAG + Documento + Providencia)
#     system_prompt = """
# Eres un asistente jur√≠dico experto en resoluci√≥n de conflictos de competencias.
# Responde SOLO con base en los documentos proporcionados.
# """

#     user_prompt = f"""
# TEXTO PROVIDENCIA BASE:
# {TEXTOPROVIDENCIA}

# DOCUMENTOS CARGADOS:
# {uploaded_text}

# DOCUMENTOS DEL √çNDICE:
# {index_context}

# PREGUNTA:
# {question}
# """

#     client = AIServices.chat_client()

#     completion = client.chat.completions.create(
#         model=settings.AZURE_OPENAI_CHAT_DEPLOYMENT,
#         messages=[
#             {"role": "system", "content": system_prompt},
#             {"role": "user", "content": user_prompt},
#         ],
#         temperature=0,
#     )

#     return {
#         "answer": completion.choices[0].message.content,
#         "citations": citations,
#     }
